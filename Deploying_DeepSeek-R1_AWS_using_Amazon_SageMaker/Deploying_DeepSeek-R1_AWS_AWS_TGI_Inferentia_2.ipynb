{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker boto3 botocore --quiet --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing with Huggingface‚Äôs Text Generation Inference (TGI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, json\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.image_uris import retrieve as sagemaker_retrieve_image_uri\n",
    "\n",
    "def select_container_image(container_selection, region_nm):\n",
    "    \"\"\"\n",
    "    Picks the container image URI (TGI or LMI) based on the choice.\n",
    "    \"\"\"\n",
    "    if container_selection == \"tgi\":\n",
    "        return \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.4.0-tgi3.0.1-gpu-py311-cu124-ubuntu22.04\"\n",
    "    elif container_selection == \"lmi\":\n",
    "        return \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124\"\n",
    "    else:\n",
    "        raise ValueError(\"Only 'tgi' or 'lmi' are allowed.\")\n",
    "\n",
    "def determine_gpu_count(hosting_type):\n",
    "    \"\"\"\n",
    "    Returns how many GPUs to allocate, based on the instance type.\n",
    "    \"\"\"\n",
    "    four_gpu_types = [\n",
    "        \"ml.g5.12xlarge\", \"ml.g6e.24xlarge\", \"ml.g6e.12xlarge\",\n",
    "        \"ml.g6.12xlarge\", \"ml.g6.24xlarge\", \"ml.g4dn.12xlarge\", \"ml.g4ad.16xlarge\"\n",
    "    ]\n",
    "    eight_gpu_types = [\n",
    "        \"ml.p5.48xlarge\", \"ml.p5e.48xlarge\", \"ml.p5en.48xlarge\",\n",
    "        \"ml.p4d.24xlarge\", \"ml.p4de.24xlarge\", \"ml.g6e.48xlarge\", \"ml.g6.48xlarge\"\n",
    "    ]\n",
    "    if hosting_type in four_gpu_types:\n",
    "        return 4\n",
    "    elif hosting_type in eight_gpu_types:\n",
    "        return 8\n",
    "    elif hosting_type.startswith(\"inf\"):\n",
    "        raise ValueError(\"Inference instance type not supported by this code.\")\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def create_sagemaker_model(img_uri, model_identifier, gpu_qty):\n",
    "    \"\"\"\n",
    "    Creates a SageMaker Model object with environment variables for the container.\n",
    "    \"\"\"\n",
    "    env_settings = {\n",
    "        \"HF_MODEL_ID\": model_identifier,\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"10000\",\n",
    "        \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.95\",\n",
    "        \"OPTION_ENABLE_STREAMING\": \"false\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"auto\",\n",
    "        \"OPTION_MODEL_LOADING_TIMEOUT\": \"3600\",\n",
    "        \"OPTION_PAGED_ATTENTION\": \"false\",\n",
    "        \"OPTION_DTYPE\": \"fp16\",\n",
    "        \"MAX_CONCURRENT_REQUESTS\": \"10\",\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "        \"SM_NUM_GPUS\": json.dumps(gpu_qty),\n",
    "    }\n",
    "    return sagemaker.Model(\n",
    "        image_uri=img_uri,\n",
    "        env=env_settings,\n",
    "        role=sagemaker.get_execution_role(),\n",
    "        name=model_identifier.split(\"/\")[-1].lower(),\n",
    "        sagemaker_session=sagemaker.Session()\n",
    "    )\n",
    "\n",
    "def deploy_sagemaker_model(sm_model, endpoint_id, inst_type):\n",
    "    \"\"\"\n",
    "    Deploys the model to a SageMaker endpoint using the given instance type.\n",
    "    \"\"\"\n",
    "    return sm_model.deploy(\n",
    "        endpoint_name=endpoint_id,\n",
    "        initial_instance_count=1,\n",
    "        instance_type=inst_type,\n",
    "        container_startup_health_check_timeout=600\n",
    "    )\n",
    "\n",
    "def test_sagemaker_endpoint(predictor_obj, user_input):\n",
    "    \"\"\"\n",
    "    Sends a quick query to check the endpoint's response.\n",
    "    \"\"\"\n",
    "    result = predictor_obj.predict({\"inputs\": user_input})\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* select_container_image() picks either TGI or LMI for inference.\n",
    "* determine_gpu_count() decides how many GPUs to use.\n",
    "* create_sagemaker_model() sets environment variables for maximum tokens, rolling batch, and more.\n",
    "* deploy_sagemaker_model() creates and deploys the endpoint.\n",
    "* test_sagemaker_endpoint() sends a prompt and prints the model‚Äôs answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Finally Configure, Deploy, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = sagemaker.Session().boto_session.region_name\n",
    "container_selection = \"tgi\"  # or \"lmi\"\n",
    "container_uri = select_container_image(container_selection, region_name)\n",
    "\n",
    "hosting_type = \"ml.g5.2xlarge\"\n",
    "gpu_num = determine_gpu_count(hosting_type)\n",
    "\n",
    "my_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model_object = create_sagemaker_model(container_uri, my_model_id, gpu_num)\n",
    "\n",
    "new_endpoint_name = (my_model_id.split(\"/\")[-1] + \"-endpoint\").lower()\n",
    "endpoint_predictor = deploy_sagemaker_model(model_object, new_endpoint_name, hosting_type)\n",
    "\n",
    "test_sagemaker_endpoint(endpoint_predictor, \"What is the meaning of life?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* container_selection decides TGI or LMI.\n",
    "* hosting_type picks the instance size.\n",
    "* gpu_num is the GPU count from determine_gpu_count().\n",
    "* create_sagemaker_model() prepares the model artifact with the container image.\n",
    "* deploy_sagemaker_model() deploys that artifact to the endpoint.\n",
    "* test_sagemaker_endpoint() sends a prompt to confirm correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferentia 2 Deployment Code\n",
    "\n",
    "For Inferentia 2, similar functions can be used, but change the container to djl-neuronx and the instance type to ml.inf2.xxx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker, json\n",
    "from sagemaker.image_uris import retrieve as sagemaker_retrieve_image_uri\n",
    "from sagemaker.model import Model\n",
    "\n",
    "# Helper: Get the inference container URI for Inferentia 2\n",
    "def get_inferentia_image(region_value):\n",
    "    return sagemaker_retrieve_image_uri(framework=\"djl-neuronx\", version=\"latest\", region=region_value)\n",
    "\n",
    "# Helper: Create a SageMaker model object with custom environment settings\n",
    "def build_inferentia_model(image_uri, hf_model_str, config_dict):\n",
    "    short_name = hf_model_str.split(\"/\")[-1].lower()\n",
    "    return Model(\n",
    "        image_uri=image_uri,\n",
    "        env=config_dict,\n",
    "        role=sagemaker.get_execution_role(),\n",
    "        name=short_name\n",
    "    )\n",
    "\n",
    "# Helper: Deploy the model to a SageMaker endpoint\n",
    "def launch_endpoint(model_obj, inst_type, suffix):\n",
    "    endpoint_id = f\"{model_obj.name}-{suffix}\"\n",
    "    model_obj.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=inst_type,\n",
    "        container_startup_health_check_timeout=1600,\n",
    "        endpoint_name=endpoint_id\n",
    "    )\n",
    "    print(f\"Deployed endpoint: {endpoint_id}\")\n",
    "    return endpoint_id\n",
    "\n",
    "# Main execution\n",
    "session_obj = sagemaker.Session()\n",
    "current_region = session_obj.boto_session.region_name\n",
    "infer_img_uri = get_inferentia_image(current_region)\n",
    "print(f\"Using inference image: {infer_img_uri}\")\n",
    "\n",
    "hf_model_identifier = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "vllm_settings = {\n",
    "    \"HF_MODEL_ID\": hf_model_identifier,\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"HF_TOKEN\": \"\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_OUTPUT_FORMATTER\": \"json\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\",\n",
    "    \"OPTION_MODEL_LOADING_TIMEOUT\": \"1600\",\n",
    "}\n",
    "\n",
    "infer_model = build_inferentia_model(infer_img_uri, hf_model_identifier, vllm_settings)\n",
    "endpoint_name = launch_endpoint(infer_model, \"ml.inf2.24xlarge\", \"ep\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test the Deployed Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Define a sample text from the ECTSum dataset (or your own sample)\n",
    "sample_text = \"\"\"\n",
    "The ECTSum dataset provides detailed articles for text summarization tasks.\n",
    "It is used to benchmark large language models for generating concise summaries.\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt that instructs the model to summarize the sample text\n",
    "prompt_template = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a summarization assistant.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Summarize the following text:\n",
    "{sample_text}\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Create a predictor for the deployed endpoint\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=session_obj,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "# Invoke the endpoint and print the output\n",
    "response = predictor.predict({\n",
    "    \"inputs\": prompt_template,\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "    }\n",
    "})\n",
    "print(response.get(\"generated_text\", \"No output returned.\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
